{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "from googlesearch import search\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "companies = pd.read_csv(\"C:/Users/hp/Desktop/DNA/training_data.csv\", header = None)\n",
    "companies = companies[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    request = urllib.request.Request(url)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    soup = BeautifulSoup(response, \"html.parser\")\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_first_url(query):\n",
    "    for url in search(query, tld=\"com.sg\", num=1, stop=1, pause=3):\n",
    "        return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_about_us(companies):\n",
    "    company_details = {}\n",
    "    for company in companies:\n",
    "        company_details[company] = {}\n",
    "        query = company +\" bloomberg snapshot\"\n",
    "        url=get_first_url(query)\n",
    "        if url[:58] == \"https://www.bloomberg.com/research/stocks/private/snapshot\":\n",
    "            soup = get_soup(url)\n",
    "            about=[]\n",
    "            add_line = soup.find(\"p\",{\"id\": \"bDesc\"})\n",
    "            #about=add_line.find(\"p\")\n",
    "            about_us=add_line.text.strip()\n",
    "            #print(company, \"\\n\",about_us) \n",
    "            company_details[company][\"About_Us\"]=about_us\n",
    "        else:\n",
    "            query = company + \" wikipedia singapore\"\n",
    "            url=get_first_url(query)\n",
    "            if url[:24]==\"https://en.wikipedia.org\":\n",
    "                try:\n",
    "                    about_us=wikipedia.summary(company)\n",
    "                    #print(company,\"\\n\",about_us)\n",
    "                    company_details[company][\"About_Us\"]=about_us\n",
    "                except:\n",
    "                    company_details[company][\"About_Us\"]=\"NA\"\n",
    "                    continue\n",
    "            else:\n",
    "                query = company +\" pitchbook\"\n",
    "                url=get_first_url(query)\n",
    "                if url[:38] == \"https://pitchbook.com/profiles/company\":\n",
    "                    soup = get_soup(url)\n",
    "                    about = []\n",
    "                    try:\n",
    "                        add_line = soup.find(\"div\", attrs={\"class\": \"XL-12 column general-info__description shift-bottom-XL-30 shift-top-XL-35\"})\n",
    "                        abt=add_line.find(\"p\")\n",
    "                        about_us = abt.text.strip()\n",
    "                        #print(company,\"\\n\", about_us)\n",
    "                        company_details[company][\"About_Us\"]=about_us\n",
    "                    except:\n",
    "                        company_details[company][\"About_Us\"]=\"NA\"\n",
    "                        continue\n",
    "                    \n",
    "                else:\n",
    "                    company_details[company][\"About_Us\"]= \"NA\"\n",
    "    return company_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_about_us(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\hp\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "abt=get_about_us(companies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
